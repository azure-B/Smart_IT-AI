from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
import torch
import os
import json
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI
import sys
import io

# [í•„ìˆ˜] Docker ë¡œê·¸ ì¶œë ¥ì„ ìœ„í•œ UTF-8 ê°•ì œ ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

# --- config ë° read_data ì„í¬íŠ¸ ---
from config import config
from read_data import extract_speaker_text_from_json_in_folder

# ==========================================
# [ì„¤ì •] Local LLM (Ollama) ì—°ê²° ì„¤ì •
# ==========================================
LOCAL_MODEL_NAME = "exaone3.5"

# Docker í™˜ê²½ë³€ìˆ˜ OLLAMA_URL ì‚¬ìš© (ì—†ìœ¼ë©´ ë¡œì»¬ ê¸°ë³¸ê°’)
default_url = "http://localhost:11434/v1"
OLLAMA_URL = os.getenv("OLLAMA_URL", default_url)

print(f"ğŸ”— AI ì—°ê²° ì£¼ì†Œ: {OLLAMA_URL}")

client = OpenAI(
    base_url=OLLAMA_URL,
    api_key="ollama"
)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜
device = "cuda" if torch.cuda.is_available() else "cpu"
model = None
dataset_embeddings = None
dataset = []

EMBEDDING_FILE = config.EMBEDDING_FILE
TEXT_DATA_FILE = config.TEXT_DATA_FILE


@app.on_event("startup")
async def startup_event():
    global model, dataset_embeddings, dataset
    print(f"ğŸš€ ì„œë²„ ì‹œì‘! ì¥ì¹˜: {device.upper()}")

    # ëª¨ë¸ ë¡œë“œ
    model = SentenceTransformer('jhgan/ko-sbert-nli', device=device)

    # ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(EMBEDDING_FILE) and os.path.exists(TEXT_DATA_FILE):
        print("--- ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘ ---")
        dataset_embeddings = torch.load(EMBEDDING_FILE, map_location=device)
        with open(TEXT_DATA_FILE, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
    else:
        print("--- ë°ì´í„° ìƒì„± ì¤‘ ---")

        # [í•µì‹¬ ìˆ˜ì •] ë°ì´í„°ì…‹ ê²½ë¡œ ìë™ íƒìƒ‰ (./Dataset ë˜ëŠ” ../Dataset)
        folder_candidates = ["Dataset", "dataset"]
        base_paths = [".", ".."]
        found_path = None

        for base in base_paths:
            for folder in folder_candidates:
                candidate = os.path.join(base, folder, "Training")
                if os.path.exists(candidate):
                    found_path = candidate
                    break
            if found_path: break

        if not found_path:
            found_path = os.path.join("./Dataset", "Training")
            print(f"âŒ ê²½ê³ : ë°ì´í„°ì…‹ í´ë”ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê²½ë¡œ í™•ì¸ í•„ìš”: {found_path}")

        dataset = extract_speaker_text_from_json_in_folder(found_path)

        if not dataset:
            print("âŒ ì˜¤ë¥˜: ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            dataset = []
        else:
            dataset_embeddings = model.encode(dataset, convert_to_tensor=True)
            torch.save(dataset_embeddings, EMBEDDING_FILE)
            with open(TEXT_DATA_FILE, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)

    print("âœ… ì¤€ë¹„ ì™„ë£Œ!")


class ChatRequest(BaseModel):
    user_input: str


@app.post("/chat")
async def chat(request: ChatRequest):
    global model, dataset_embeddings, dataset

    query = request.user_input.strip()
    if not query:
        return {"reply": "ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.", "score": 0.0, "is_generated": False}

    if dataset_embeddings is None or len(dataset) == 0:
        return {"reply": "ì„œë²„ ì´ˆê¸°í™” ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.", "score": 0.0, "is_generated": False}

    # 1. [ê²€ìƒ‰] (Retrieval)
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, dataset_embeddings, top_k=1)

    top_hit = hits[0][0]
    matched_text = dataset[top_hit['corpus_id']]
    score = top_hit['score']

    if "ë‹µë³€:" in matched_text:
        reference_answer = matched_text.split("ë‹µë³€:", 1)[1].strip()
    else:
        reference_answer = matched_text

    # 2. [ìƒì„±] (Generation)
    print(f"ğŸ¤– {LOCAL_MODEL_NAME}ì—ê²Œ ìƒì„± ìš”ì²­ ì¤‘...")

    try:
        completion = client.chat.completions.create(
            model=LOCAL_MODEL_NAME,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. "
                        "ì œê³µëœ [ì°¸ê³  ì •ë³´] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. "
                        "ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , 3~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë”°ëœ»í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”."
                        "ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”."
                    )
                },
                {
                    "role": "user",
                    "content": f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n[ì°¸ê³  ì •ë³´]: {reference_answer}"
                }
            ],
            temperature=0.7
        )
        final_answer = completion.choices[0].message.content
        is_generated = True

    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        final_answer = reference_answer
        is_generated = False

    return {
        "reply": final_answer,
        "score": float(score),
        "is_generated": is_generated
    }


# [ìˆ˜ì •] Node.js(8008)ì™€ ê²¹ì¹˜ì§€ ì•Šê²Œ 5000ë²ˆ í¬íŠ¸ë¡œ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=5000)