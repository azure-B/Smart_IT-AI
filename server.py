from fastapi import FastAPI
from pydantic import BaseModel
import uvicorn
import torch
import os
import json
from sentence_transformers import SentenceTransformer, util
from openai import OpenAI  # [ì¶”ê°€] Ollama ì—°ê²°ìš©

# --- ê¸°ì¡´ config ë° read_data ì„í¬íŠ¸ ---
from config import config
from read_data import extract_speaker_text_from_json_in_folder

# ==========================================
# [ì„¤ì •] Local LLM (Ollama) ì—°ê²° ì„¤ì •
# ==========================================
# 1. ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (í„°ë¯¸ë„ì—ì„œ 'ollama pull exaone3.5' ë¯¸ë¦¬ ì‹¤í–‰ í•„ìš”)
LOCAL_MODEL_NAME = "exaone3.5"

# 2. Ollama ì£¼ì†Œ ì„¤ì •
# Dockerì—ì„œ ì‹¤í–‰ ì‹œ -e OLLAMA_URL="..." ì˜µì…˜ìœ¼ë¡œ ì£¼ì…ëœ ê°’ì„ ì‚¬ìš©
# ê°’ì´ ì—†ìœ¼ë©´ ë¡œì»¬ ê¸°ë³¸ê°’(localhost) ì‚¬ìš©
default_url = "http://localhost:11434/v1"
OLLAMA_URL = os.getenv("OLLAMA_URL", default_url)

print(f"ğŸ”— AI ì—°ê²° ì£¼ì†Œ: {OLLAMA_URL}")

# Ollama í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url=OLLAMA_URL,
    api_key="ollama"
)

app = FastAPI()

# ì „ì—­ ë³€ìˆ˜ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
model = None
dataset_embeddings = None
dataset = []

EMBEDDING_FILE = config.EMBEDDING_FILE
TEXT_DATA_FILE = config.TEXT_DATA_FILE


@app.on_event("startup")
async def startup_event():
    global model, dataset_embeddings, dataset
    print(f"ğŸš€ ì„œë²„ ì‹œì‘! ì¥ì¹˜: {device.upper()}")

    # ëª¨ë¸ ë¡œë“œ
    model = SentenceTransformer('jhgan/ko-sbert-nli', device=device)

    # ë°ì´í„° ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(EMBEDDING_FILE) and os.path.exists(TEXT_DATA_FILE):
        print("--- ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘ ---")
        dataset_embeddings = torch.load(EMBEDDING_FILE, map_location=device)
        with open(TEXT_DATA_FILE, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
    else:
        print("--- ë°ì´í„° ìƒì„± ì¤‘ ---")
        test_path = os.path.join("dataset", "Training")
        dataset = extract_speaker_text_from_json_in_folder(test_path)

        if not dataset:
            print("âŒ ì˜¤ë¥˜: ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        dataset_embeddings = model.encode(dataset, convert_to_tensor=True)
        torch.save(dataset_embeddings, EMBEDDING_FILE)
        with open(TEXT_DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(dataset, f, ensure_ascii=False, indent=2)

    print("âœ… ì¤€ë¹„ ì™„ë£Œ!")


class ChatRequest(BaseModel):
    user_input: str


@app.post("/chat")
async def chat(request: ChatRequest):
    global model, dataset_embeddings, dataset

    query = request.user_input

    # 1. [ê²€ìƒ‰] SBERTë¡œ ê°€ì¥ ìœ ì‚¬í•œ ë‹µë³€ ì°¾ê¸° (Retrieval)
    query_embedding = model.encode(query, convert_to_tensor=True)
    hits = util.semantic_search(query_embedding, dataset_embeddings, top_k=1)

    top_hit = hits[0][0]
    matched_text = dataset[top_hit['corpus_id']]
    score = top_hit['score']

    # ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œ (Contextë¡œ ì‚¬ìš©)
    if "ë‹µë³€:" in matched_text:
        reference_answer = matched_text.split("ë‹µë³€:", 1)[1].strip()
    else:
        reference_answer = matched_text

    # 2. [ìƒì„±] Ollamaì—ê²Œ ë‹µë³€ ìš”ì•½ ìš”ì²­ (Generation)
    print(f"ğŸ¤– {LOCAL_MODEL_NAME}ì—ê²Œ ìƒì„± ìš”ì²­ ì¤‘...")

    try:
        completion = client.chat.completions.create(
            model=LOCAL_MODEL_NAME,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ìˆ˜ì˜ì‚¬ì…ë‹ˆë‹¤. "
                        "ì œê³µëœ [ì°¸ê³  ì •ë³´] ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. "
                        "ì „ë¬¸ ìš©ì–´ëŠ” ì‰½ê²Œ í’€ì–´ì„œ ì„¤ëª…í•˜ê³ , 3~4ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ë§Œ ìš”ì•½í•´ì„œ ë”°ëœ»í•˜ê²Œ ë§í•´ì£¼ì„¸ìš”."
                        "ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ˆì„¸ìš”."
                    )
                },
                {
                    "role": "user",
                    "content": f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\n[ì°¸ê³  ì •ë³´]: {reference_answer}"
                }
            ],
            temperature=0.7
        )
        final_answer = completion.choices[0].message.content
        is_generated = True

    except Exception as e:
        print(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë‹µë³€ ë°˜í™˜
        final_answer = reference_answer
        is_generated = False

    return {
        "reply": final_answer,
        "score": float(score),
        "is_generated": is_generated  # ìƒì„± ì—¬ë¶€ë¥¼ í´ë¼ì´ì–¸íŠ¸ê°€ ì•Œ ìˆ˜ ìˆê²Œ ì¶”ê°€
    }

# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš©
# if __name__ == "__main__":
#     uvicorn.run(app, host="0.0.0.0", port=8000)